import mediapipe as mp
import cv2
import math
import osascript
import numpy as np

Hands = mp.solutions.hands.Hands(min_detection_confidence=0.7)
draw = mp.solutions.drawing_utils
cap = cv2.VideoCapture(0)

while True:
    success, img = cap.read()

    if success:
        img = cv2.flip(img, 1)
        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = Hands.process(imgRGB)
        lmList = []

        if results.multi_hand_landmarks:
            for handLMS in results.multi_hand_landmarks:
                for id, lm in enumerate(handLMS.landmark):
                    h, w, c = img.shape
                    lmList.append([id, int(lm.x * w), int(lm.y * h)])
                    
                    # uncomment to show all landmarks (21 points)
                    # draw.draw_landmarks(img, handLMS, mp.solutions.hands.HAND_CONNECTIONS)

                if len(lmList) != 0:
                    x2 = lmList[4]
                    x1 = lmList[8]
                    dist = math.sqrt(pow(x2[1] - x1[1], 2) + pow(x2[2] - x1[2], 2))

                    # draw a circle at fingertips and line connecting the 2 fingertips
                    cv2.line(img, (x1[1], x1[2]), (x2[1], x2[2]), (255, 0, 255), 2)
                    cv2.circle(img, (x1[1], x1[2]), 7, (255, 0, 255), cv2.FILLED)
                    cv2.circle(img, (x2[1], x2[2]), 7, (255, 0, 255), cv2.FILLED)
                    
                    # set device volume (macOS) and show the same on output window
                    target_volume = np.interp(dist, [50, 280], [0, 100])
                    cv2.putText(img, f'Volume: {int(target_volume)}%', (50, 150),
                                cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 2)
                    osascript.osascript("set volume output volume {}".format(target_volume))

        cv2.imshow("Webcam input", img)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    else:
        break
